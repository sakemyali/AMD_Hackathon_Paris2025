
# ‚òïü•ê KACM ‚Äì Coffee & Croissant Robot Manipulation

**AMD Robotics Hackathon 2025**

**Team:** KACM

---

## Overview

This repository contains our submission for the **AMD Robotics Hackathon 2025**.

Our objective was to train a real robot to perform **object-aware manipulation tasks** in a household-style setting, including:

* Picking up a **croissant**
* Dipping it carefully into a cup of **coffee**
* Ignoring non-target objects
* Picking up a **baguette** and handing it safely to a human

We explored **two learning-based robotic policies independently** using the **LeRobot framework**:

* **ACT (Action Chunking Transformers)** ‚Äì imitation learning for manipulation
* **Vision-Language-Action (VLA) models** (**œÄ0, SmolVLA**) ‚Äì object selection and semantic grounding

Each policy was **trained and evaluated separately** on the same robotic setup and dataset.

---

## Project Goals

* Demonstrate **precise real-world manipulation**
* Enable **object-aware behavior** (croissant vs baguette vs other items)
* Evaluate different policy families on the **same real task**
* Use **teleoperated demonstrations** with real robot hardware

---

## Robot Setup

* **Robot:** SO-101 follower arm + leader arm (teleoperation)
* **Cameras (3 views):**

  | Camera  | Purpose                            |
  | ------- | ---------------------------------- |
  | Gripper | Fine grasping and manipulation     |
  | Top     | Workspace overview                 |
  | Side    | Depth cues and collision awareness |
* **Resolution:** 640√ó480
* **Frame Rate:** 30 FPS

---

## Learning Approaches

### 1. ACT ‚Äì Action Chunking Transformers

* **Type:** Imitation Learning
* **Inputs:**

  * Multi-view RGB images
  * Robot joint states
* **Outputs:**

  * Action chunks (joint-level control)
* **Strengths:**

  * Smooth, precise manipulation
  * Reliable execution of pick, dip, and handover motions
* **Limitations:**

  * No semantic understanding of objects

ACT was used to **learn full manipulation trajectories** directly from human teleoperation.

---

### 2. Vision-Language-Action Models (œÄ0, SmolVLA)

* **Type:** Vision-Language-Action (VLA)
* **Inputs:**

  * Multi-view images
  * Optional language prompts (e.g., ‚Äúpick the croissant‚Äù)
* **Outputs:**

  * Object-conditioned actions
* **Strengths:**

  * Semantic object discrimination
  * Can differentiate croissant, baguette, and other objects
* **Limitations:**

  * Less precise motion compared to ACT

VLA models were used to **evaluate object selection and grounding**, not as a combined controller with ACT.

---

## Important Clarification

> **ACT and œÄ0 / SmolVLA were not combined into a single multi-stage or hierarchical policy.**

They were:

* Trained independently
* Evaluated independently
* Compared on the same robotic task

---

## Missions

### Mission 1 ‚Äì Preliminary Task

A simple pick-and-place task used to:

* Validate the LeRobot pipeline
* Test data recording and inference
* Calibrate the robot and cameras
---------------- add video ----------------
**Mission 1 Artifacts:**

* üì¶ Dataset:
  [https://huggingface.co/datasets/sakemyali/mission1-2](https://huggingface.co/datasets/sakemyali/mission1-2)
* ü§ñ Model (ACT):
  [https://huggingface.co/sakemyali/try2](https://huggingface.co/sakemyali/try2)

---

### Mission 2 ‚Äì Coffee & Croissant Manipulation

## Submission Details

### 1. Mission Description

**Real-world motivation:**
Household and assistive robots must safely manipulate everyday food items while understanding *what* to pick and *what to ignore*. Tasks such as serving food, handing objects to humans, or interacting with fragile items require a combination of **precise manipulation**, **object awareness**, and **robust execution in unstructured environments**.

Our robot autonomously performs the following behaviors:

* Detect and pick a **croissant**
* Dip it carefully into a **cup of coffee**
* Release the croissant safely
* Detect and pick a **baguette**
* Hand it over to a human
* Ignore unrelated or forbidden objects (e.g., medicine)
* Remain idle when no target object is present

With prior experience building classical manipulation pipelines, this project represents a transition toward **learning-based robotic control**, relying on data-driven policies trained from real demonstrations.

---

### 2. Creativity

* **Yes/No object acknowledgment**
  The robot responds semantically to objects:

  * Performs actions and signals ‚Äúyes‚Äù for target objects (croissant, baguette)
  * Refuses and signals ‚Äúno‚Äù for forbidden objects (doliprane)

* **Movement repetition**
  Each action is performed twice during training to:

  * Reinforce learning of each sub-action (pick, dip, handover)
  * Improve recovery from disturbances or minor errors

* **High-quality teleoperation demonstrations**
  We carefully teleoperated the SO-101 robot in regions of high manipulability, producing demonstrations that are smooth, realistic, and reliable:

  * Smooth trajectories with minimal abrupt motions
  * Natural grasping motions that preserve object integrity
  * Fewer failed trials for higher-quality training data

* **Soft-object manipulation strategy**
  The croissant is delicate and deformable, so we implemented:

  * Controlled wrist orientation to match the croissant‚Äôs shape
  * Slow approach speeds to prevent slipping or deformation

* **Safe handover behavior**
  During baguette delivery:

  * Conservative joint limits prevent sudden or unsafe motions
  * Reduced velocity ensures human safety
  * Demonstrates reliable human-robot interaction

* **Multi-view perception setup (3 synchronized cameras)**

  * **Gripper view:** monitors fine manipulation and object contact
  * **Top view:** provides workspace context and tracks object positions
  * **Side view:** gives depth cues and ensures collision-free motion

* **Robustness to disturbances**
  Policies tolerate real-world variability, including:

  * Minor object shifts or slips
  * Partial occlusions
  * Lighting changes such as shadows or dim environments

* **Generalizability**
  Learned policies can handle unseen objects safely:

  * Correctly interacts with new objects (e.g., a pencil)
  * Adapts to new shapes and sizes with minimal extra demonstrations


---

### 3. Technical Implementation

#### Training

* **Policies (trained independently):**

  * **ACT (Action Chunking Transformers)** for manipulation execution
  * **Vision-Language-Action models (œÄ0, SmolVLA)** for object selection and semantic grounding

* **Inputs:**

  * Multi-view RGB images
  * Robot joint positions

* **Outputs:**

  * Joint-level action commands (for pi0) or action chunks (for the other policies)

---

#### Teleoperation & Dataset Capture

* **Robot:** SO-101 follower / leader arm
* **Cameras:**

  * 3√ó USB RGB cameras
  * Resolution: 640√ó480
  * Frame rate: 30 FPS

The dataset follows the **LeRobot standard format** and is reusable for future tasks.

---

#### Scripts Included (`mission2/code`)

* `fix_arms_access_permission.sh`
* `calib_follower.sh`
* `calib_leader.sh`
* `record_teleop_data_640x480_30fps_3cams.sh`
* `inference_teleop_640x480_30fps_3cams.sh`

---

### 4. Inference & Evaluation

* Same real-world hardware setup used during training
* Fully autonomous execution after start signal
* No human intervention during task execution
* Evaluated across multiple trials and lighting conditions
---------------------add training plots ----------
---

### 5. Results
---------------------------- add videos -------------
#### 1. Successful Full-Task Execution

The robot reliably completes:

* Croissant pick and dip
* Baguette pick and handover

across repeated trials.

---

#### 2. Object Awareness & Filtering

* Correctly distinguishes croissant, baguette, and non-target objects
* Avoids forbidden items
* Remains idle when no target object is detected

---

#### 3. Robustness to Lighting Conditions

Performance remains stable under:

* Bright lighting
* Dim indoor lighting
* Partial shadows

---

#### 4. Failure Handling & Recovery

* If a grasp fails, the robot reattempts the action
* If the object slips, the policy recovers without reset
* Small disturbances do not cause task failure

---

### 6. Ease of Use

* Generalizable to new objects with additional demonstrations
* Modular policy design using LeRobot
* Simple execution workflow:

  * **Data collection:** `./record_*.sh <dataset-name>`
  * **Training:** single LeRobot notebook
  * **Inference:** `./inference_*.sh`

---

### 7. Future Extensions

* Multi-object serving scenarios
* Integration of VLA for object selection + ACT for execution
* Adaptation to home assistance or caf√© service environments


---

## Dataset

* Collected via **teleoperation**
* Multi-camera synchronized recordings
* Smooth and natural human demonstrations
* Compatible with LeRobot standard format

**Mission 2 Dataset:**

* üì¶ [https://huggingface.co/datasets/sakemyali/m2set](https://huggingface.co/datasets/sakemyali/m2set)

---

## Trained Models

* **ACT (Imitation Learning):**

  * Learns manipulation trajectories
* **œÄ0 / SmolVLA (VLA):**

  * Learns object-aware behavior via vision-language grounding

**Models:**

* ü§ñACT_v1 [https://huggingface.co/KACM/act_pi0-croissant-so101](https://huggingface.co/KACM/act_pi0-croissant-so101)

* ü§ñACT_v2 [https://huggingface.co/sakemyali/actm2_v2](https://huggingface.co/sakemyali/actm2_v2)

* ü§ñsmolVLA [https://huggingface.co/sakemyali/svlam2](https://huggingface.co/sakemyali/svlam2)

* ü§ñpi0 [https://huggingface.co/sakemyali/p0m2](https://huggingface.co/sakemyali/p0m2)

---

## Repository Structure

```bash
coffee-croissant-robot/
‚îú‚îÄ‚îÄ mission1/
‚îÇ   ‚îî‚îÄ‚îÄ code/
‚îú‚îÄ‚îÄ mission2/
‚îÇ   ‚îî‚îÄ‚îÄ code/
‚îÇ       ‚îú‚îÄ‚îÄ calib_follower.sh
‚îÇ       ‚îú‚îÄ‚îÄ calib_leader.sh
‚îÇ       ‚îú‚îÄ‚îÄ fix_arms_access_permission.sh
‚îÇ       ‚îú‚îÄ‚îÄ record_teleop_data_640x480_30fps_3cams.sh
‚îÇ       ‚îú‚îÄ‚îÄ inference_teleop_640x480_30fps_3cams.sh
‚îú‚îÄ‚îÄ README.md
```

---

## How to Reproduce

### 1. Environment Setup

```bash
conda create -n lerobot python=3.10
conda activate lerobot
pip install lerobot
```

Install camera utilities:

```bash
sudo apt-get update
sudo apt-get install -y v4l-utils
```

---

### 2. Hugging Face Login

```bash
export HF_USER="your_username"
export HUGGINGFACE_TOKEN="hf_xxxxxxxxxxxx"
cd mission2/code
./login_huggingface.sh
```

---

### 3. Fix Serial Permissions

```bash
./fix_arms_access_permission.sh
```

---

### 4. Calibrate the Arms

```bash
./calib_follower.sh
./calib_leader.sh
```

---

### 5. Record Teleoperation Data

```bash
./record_teleop_data_640x480_30fps_3cams.sh croissant-dipping-so101-v1
```

---

### 6. Train Policies

* Use LeRobot notebooks
* Train **ACT** and **œÄ0 / SmolVLA** independently on the same dataset

---

### 7. Run Inference

```bash
./inference_teleop_640x480_30fps_3cams.sh
```

---

## Results

* ‚úÖ Successful croissant dipping
* ‚úÖ Safe baguette handover
* ‚úÖ Robust object discrimination
* ‚úÖ Stable performance under different lighting conditions

------------------Add videos + training plots -----------------

---

## Limitations & Future Work

* ACT lacks semantic object understanding
* VLA models lack high-precision control
* Future work could combine:

  * VLA for object selection
  * ACT for motion execution
* Extend to more objects and tasks

---

## Conclusion

This project demonstrates how **different policy families** can solve complementary aspects of a real-world manipulation problem.

By evaluating **ACT and Vision-Language-Action models independently**, we highlight their respective strengths and limitations in a practical robotic scenario.

---

## Acknowledgements

* AMD Robotics Hackathon 2025
* LeRobot framework
* Hugging Face Datasets & Models

---


